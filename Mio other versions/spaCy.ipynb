{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ehTizmGwk1G",
        "outputId": "42403f8f-211b-47d6-bf54-fa1c71126b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn-hierarchical-classification in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.10/dist-packages (from sklearn-hierarchical-classification) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from sklearn-hierarchical-classification) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-hierarchical-classification) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from sklearn-hierarchical-classification) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.0->sklearn-hierarchical-classification) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.0->sklearn-hierarchical-classification) (3.2.0)\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install sklearn-hierarchical-classification\n",
        "!pip install scikit-multilearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
        "from sklearn_hierarchical_classification.constants import ROOT\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skmultilearn.adapt import MLkNN\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
        "from sklearn_hierarchical_classification.constants import ROOT\n",
        "from networkx import DiGraph\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import csr_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S40PN27VwqHv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# serie di 4 funzioni per la suddivisione del dataset secondo etichette\n",
        "def find_descendants(graph, labels):\n",
        "    descendants = set()\n",
        "\n",
        "    def dfs(node):\n",
        "        descendants.add(node)\n",
        "        [dfs(child) for child in graph.successors(node)]\n",
        "\n",
        "    [dfs(label) for label in labels]\n",
        "    return descendants\n",
        "\n",
        "def filter_matrix_by_labels(X, y, labels, graph):\n",
        "    descendants_set = find_descendants(graph, labels)\n",
        "\n",
        "    filtered_X, filtered_labels = zip(*[(X[i], list(set(labels_list) & descendants_set)) for i, labels_list in enumerate(y) if set(labels_list) & descendants_set])\n",
        "\n",
        "    return np.vstack(filtered_X), list(filtered_labels)\n",
        "\n",
        "def find_ancestor_in_labels(graph, node, target_labels):\n",
        "    ancestors = set()\n",
        "\n",
        "    def dfs(current):\n",
        "        ancestors.add(current)\n",
        "        [dfs(pred) for pred in graph.predecessors(current)]\n",
        "\n",
        "    [dfs(predecessor) for predecessor in graph.predecessors(node)]\n",
        "\n",
        "    common_ancestors = ancestors & set(target_labels)\n",
        "    return list(common_ancestors)[0] if common_ancestors else None\n",
        "\n",
        "def transform_Y_with_ancestors(graph, Y, target_labels):\n",
        "    return [list(set([find_ancestor_in_labels(graph, label, target_labels) or label for label in labels_list])) for labels_list in Y]\n",
        "###############################\n",
        "\n",
        "\n",
        "def fit_classifier(X_Train, y_Train, G, labels_to_check, classifier, nlp):\n",
        "    X, Y = filter_matrix_by_labels(X_train, y_train, labels_to_check, G)\n",
        "    new_Y = transform_Y_with_ancestors(G, Y, labels_to_check)\n",
        "    Y = Binarizzatore_etichette(labels_to_check, new_Y)\n",
        "    #X_tfidf = vectorizer.transform(X.flatten().tolist())\n",
        "    X = [doc.vector for doc in nlp.pipe(X.flatten().tolist())]\n",
        "    X = np.vstack([vec if vec.size > 0 else np.zeros(96) for vec in X])\n",
        "\n",
        "    return classifier.fit(X, Y)\n",
        "\n",
        "\n",
        "def Binarizzatore_etichette(lista_stringhe, lista_liste):\n",
        "    # Inizializza la matrice con zeri\n",
        "    matrice = [[0] * len(lista_stringhe) for _ in range(len(lista_liste))]\n",
        "\n",
        "    # Riempie la matrice con 1 dove una stringa è presente nella lista di liste\n",
        "    for i, lista in enumerate(lista_liste):\n",
        "        for j, stringa in enumerate(lista_stringhe):\n",
        "            if stringa in lista:\n",
        "                matrice[i][j] = 1\n",
        "\n",
        "    return matrice\n",
        "\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = json.load(file)\n",
        "        return data\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def check_errors(graph, binary_vector, predicted_labels, true_labels):\n",
        "    for pred_label, pred_value in zip(predicted_labels, binary_vector):\n",
        "        if pred_value == 1 and not any(nx.has_path(graph.reverse(), label, pred_label) for label in true_labels):\n",
        "            return False\n",
        "        elif pred_value == 0 and pred_label in true_labels:\n",
        "            return False\n",
        "\n",
        "    return True  # Se non ci sono errori, restituisce True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "metadata": {
        "id": "Dm9YWEVZ66gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfsJ6sE_wqQF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Construct the graph\n",
        "G = DiGraph()\n",
        "G.add_edge(ROOT, \"Free\")\n",
        "G.add_edge(ROOT, \"Persuasion\")\n",
        "G.add_edge(\"Persuasion\", \"Logos\")\n",
        "G.add_edge(\"Logos\", \"Repetition\")\n",
        "G.add_edge(\"Logos\", \"Obfuscation, Intentional vagueness, Confusion\")\n",
        "G.add_edge(\"Logos\", \"Reasoning\")\n",
        "G.add_edge(\"Logos\", \"Justification\")\n",
        "G.add_edge('Justification', \"Slogans\")\n",
        "G.add_edge('Justification', \"Bandwagon\")\n",
        "G.add_edge('Justification', \"Appeal to authority\")\n",
        "G.add_edge('Justification', \"Flag-waving\")\n",
        "G.add_edge('Justification', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Reasoning', \"Simplification\")\n",
        "G.add_edge('Simplification', \"Causal Oversimplification\")\n",
        "G.add_edge('Simplification', \"Black-and-white Fallacy/Dictatorship\")\n",
        "G.add_edge('Simplification', \"Thought-terminating cliché\")\n",
        "G.add_edge('Reasoning', \"Distraction\")\n",
        "G.add_edge('Distraction', \"Misrepresentation of Someone's Position (Straw Man)\")\n",
        "G.add_edge('Distraction', \"Presenting Irrelevant Data (Red Herring)\")\n",
        "G.add_edge('Distraction', \"Whataboutism\")\n",
        "G.add_edge(\"Persuasion\", \"Ethos\")\n",
        "G.add_edge('Ethos', \"Appeal to authority\")\n",
        "G.add_edge('Ethos', \"Glittering generalities (Virtue)\")\n",
        "G.add_edge('Ethos', \"Bandwagon\")\n",
        "G.add_edge('Ethos', \"Ad Hominem\")\n",
        "G.add_edge('Ad Hominem', \"Doubt\")\n",
        "G.add_edge('Ad Hominem', \"Name calling/Labeling\")\n",
        "G.add_edge('Ad Hominem', \"Smears\")\n",
        "G.add_edge('Ad Hominem', \"Reductio ad hitlerum\")\n",
        "G.add_edge('Ad Hominem', \"Whataboutism\")\n",
        "G.add_edge(\"Persuasion\", \"Pathos\")\n",
        "G.add_edge('Pathos', \"Exaggeration/Minimisation\")\n",
        "G.add_edge('Pathos', \"Loaded Language\")\n",
        "G.add_edge('Pathos', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Pathos', \"Flag-waving\")\n",
        "\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "# Leggi train.json\n",
        "train_data = read_json_file(\"train.json\")\n",
        "\n",
        "# Leggi validation.json\n",
        "validation_data = read_json_file(\"validation.json\")\n",
        "\n",
        "test_data = read_json_file(\"dev_subtask1_en.json\")\n",
        "\n",
        "\n",
        "# Inizializza le liste per i dati di addestramento\n",
        "texts_train, labels_train = [], []\n",
        "texts_validation, labels_validation = [], []\n",
        "texts_test, labels_test = [], []\n",
        "\n",
        "if train_data:\n",
        "    texts_train = [element['text'] for element in train_data]\n",
        "    labels_train = [element['labels'] if element['labels'] else ['Free'] for element in train_data]\n",
        "\n",
        "# Estre x e y dai dataset se esistono\n",
        "if validation_data:\n",
        "    texts_validation = [element['text'] for element in validation_data]\n",
        "    labels_validation = [element['labels'] if element['labels'] else ['Free'] for element in validation_data]\n",
        "\n",
        "if test_data:\n",
        "    texts_test = [element['text'] for element in test_data]\n",
        "    labels_test = [element['labels'] if element['labels'] else ['Free'] for element in test_data]\n",
        "\n",
        "# Unisce i dati di addestramento e validation\n",
        "texts = texts_train + texts_validation\n",
        "labels = labels_train + labels_validation\n",
        "\n",
        "####### nuova sezione per avere validation.json come gold\n",
        "X_test = texts_test\n",
        "y_test = labels_test\n",
        "X_train = texts\n",
        "y_train = labels\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "X_test = [doc.vector for doc in nlp.pipe(X_test)]\n",
        "X_test = np.vstack(X_test)\n",
        "\n",
        "# Definizione dei classificatori\n",
        "Free_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "Persuasion_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "Ethos_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "Ad_hominem_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "Pathos_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "Logos_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "Justification_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "Reasoning_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "Distraction_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "Simplification_classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "#addestramento dei classificatori tramite apposita funzione che gli permette di usare solo il sottoinsieme del dataset che gli serve\n",
        "Free_classifier = fit_classifier(X_train, y_train, G, [\"Free\", \"Persuasion\"], Free_classifier, nlp)\n",
        "Persuasion_classifier = fit_classifier(X_train, y_train, G, [\"Ethos\", \"Pathos\", \"Logos\"], Persuasion_classifier, nlp)\n",
        "Ethos_classifier = fit_classifier(X_train, y_train, G, [\"Ad Hominem\", \"Bandwagon\", \"Glittering generalities (Virtue)\" , \"Appeal to authority\"], Ethos_classifier, nlp)\n",
        "Ad_hominem_classifier = fit_classifier(X_train, y_train, G, [\"Doubt\", \"Name calling/Labeling\", \"Smears\", \"Reductio ad hitlerum\",\"Whataboutism\"], Ad_hominem_classifier, nlp)\n",
        "Pathos_classifier = fit_classifier(X_train, y_train, G, [\"Exaggeration/Minimisation\", \"Loaded Language\", \"Appeal to fear/prejudice\", \"Flag-waving\"], Pathos_classifier, nlp)\n",
        "Logos_classifier = fit_classifier(X_train, y_train, G, [\"Repetition\", \"Obfuscation, Intentional vagueness, Confusion\", \"Reasoning\", \"Justification\"], Logos_classifier, nlp)\n",
        "Justification_classifier = fit_classifier(X_train, y_train, G, [\"Slogans\", \"Bandwagon\", \"Appeal to authority\", \"Flag-waving\", \"Appeal to fear/prejudice\"], Justification_classifier, nlp)\n",
        "Reasoning_classifier = fit_classifier(X_train, y_train, G, [\"Simplification\", \"Distraction\"], Reasoning_classifier, nlp)\n",
        "Distraction_classifier = fit_classifier(X_train, y_train, G, [\"Whataboutism\",\"Misrepresentation of Someone's Position (Straw Man)\",\"Presenting Irrelevant Data (Red Herring)\"], Distraction_classifier, nlp)\n",
        "Simplification_classifier = fit_classifier(X_train, y_train, G, [\"Causal Oversimplification\", \"Black-and-white Fallacy/Dictatorship\", \"Thought-terminating cliché\"], Simplification_classifier, nlp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOsYvGF5kOuw"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "\n",
        "# primo carattere conta il numero di errori, il seocndo il numero di volte che viene usato il classificatore\n",
        "counter_Free = [0, X_test.shape[0]]\n",
        "counter_Persuasion = [0, 0]\n",
        "counter_Ethos = [0, 0]\n",
        "counter_Ad_hominem = [0, 0]\n",
        "counter_Pathos = [0, 0]\n",
        "counter_Logos = [0, 0]\n",
        "counter_Justification = [0, 0]\n",
        "counter_Reasoning = [0, 0]\n",
        "counter_Distraction = [0, 0]\n",
        "counter_Simplification = [0, 0]\n",
        "\n",
        "X_test = csr_matrix(X_test)\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    y_pred.append([])\n",
        "\n",
        "    #probabilities = Free_classifier.predict_proba(X_test[i])\n",
        "    prediction = Free_classifier.predict(X_test[i])\n",
        "    counter_Free[0] += not check_errors(G, prediction[0], [\"Free\", \"Persuasion\"], y_test[i])\n",
        "\n",
        "    # Free\n",
        "    if prediction[0, 0] == 1:\n",
        "        #y_pred[i].append(\"Free\")\n",
        "        continue\n",
        "\n",
        "    # Persuasion\n",
        "    if prediction[0, 1] == 1:\n",
        "        prediction = Persuasion_classifier.predict(X_test[i])\n",
        "        counter_Persuasion[1] += 1\n",
        "        counter_Persuasion[0] += not check_errors(G, prediction[0], [\"Ethos\", \"Pathos\", \"Logos\"], y_test[i])\n",
        "\n",
        "        #Ethos\n",
        "        if prediction[0, 0] == 1:\n",
        "            prediction1 = Ethos_classifier.predict(X_test[i])\n",
        "            counter_Ethos[1] += 1\n",
        "            counter_Ethos[0] += not check_errors(G, prediction1[0], [\"Ad Hominem\", \"Bandwagon\", \"Glittering generalities (Virtue)\" , \"Appeal to authority\"], y_test[i])\n",
        "\n",
        "            # Ad Hominem\n",
        "            if prediction1[0, 0] == 1:\n",
        "                prediction8 = Ad_hominem_classifier.predict(X_test[i])\n",
        "                counter_Ad_hominem[1] += 1\n",
        "                counter_Ad_hominem[0] += not check_errors(G, prediction8[0], [\"Doubt\", \"Name calling/Labeling\", \"Smears\", \"Reductio ad hitlerum\",\"Whataboutism\"], y_test[i])\n",
        "                if prediction8[0, 0] == 1:\n",
        "                    y_pred[i].append(\"Doubt\")\n",
        "                if prediction8[0, 1] == 1:\n",
        "                    y_pred[i].append(\"Name calling/Labeling\")\n",
        "                if prediction8[0, 2] == 1:\n",
        "                    y_pred[i].append(\"Smears\")\n",
        "                if prediction8[0, 3] == 1:\n",
        "                    y_pred[i].append(\"Reductio ad hitlerum\")\n",
        "                if prediction8[0, 4] == 1:\n",
        "                    y_pred[i].append(\"Whataboutism\")\n",
        "                if all(prediction8[0, i] == 0 for i in range(5)):\n",
        "                    y_pred[i].append(\"Ad Hominem\")\n",
        "\n",
        "            if prediction1[0, 1] == 1:\n",
        "                y_pred[i].append(\"Bandwagon\")\n",
        "            if prediction1[0, 2] == 1:\n",
        "                y_pred[i].append(\"Glittering generalities (Virtue)\")\n",
        "            if prediction1[0, 3] == 1:\n",
        "                y_pred[i].append(\"Appeal to authority\")\n",
        "            if all(prediction1[0, i] == 0 for i in range(4)):\n",
        "                y_pred[i].append(\"Ethos\")\n",
        "        #Pathos\n",
        "        if prediction[0, 1] == 1:\n",
        "            prediction2 = Pathos_classifier.predict(X_test[i])\n",
        "            counter_Pathos[1] += 1\n",
        "            counter_Pathos[0] += not check_errors(G, prediction2[0], [\"Exaggeration/Minimisation\", \"Loaded Language\", \"Appeal to fear/prejudice\", \"Flag-waving\"], y_test[i])\n",
        "            if prediction2[0, 0] == 1:\n",
        "                y_pred[i].append(\"Exaggeration/Minimisation\")\n",
        "            if prediction2[0, 1] == 1:\n",
        "                y_pred[i].append(\"Loaded Language\")\n",
        "            if prediction2[0, 2] == 1:\n",
        "                y_pred[i].append(\"Appeal to fear/prejudice\")\n",
        "            if prediction2[0, 3] == 1:\n",
        "                y_pred[i].append(\"Flag-waving\")\n",
        "            if all(prediction2[0, i] == 0 for i in range(4)):\n",
        "                y_pred[i].append(\"Pathos\")\n",
        "        #Logos\n",
        "        if prediction[0, 2] == 1:\n",
        "            prediction3 = Logos_classifier.predict(X_test[i])\n",
        "            counter_Logos[1] += 1\n",
        "            counter_Logos[0] += not check_errors(G, prediction3[0], [\"Repetition\", \"Obfuscation, Intentional vagueness, Confusion\", \"Reasoning\", \"Justification\"], y_test[i])\n",
        "            if prediction3[0, 0] == 1:\n",
        "                y_pred[i].append(\"Repetition\")\n",
        "            if prediction3[0, 1] == 1:\n",
        "                y_pred[i].append(\"Obfuscation, Intentional vagueness, Confusion\")\n",
        "\n",
        "            # Reasoning\n",
        "            if prediction3[0, 2] == 1:\n",
        "                prediction4 = Reasoning_classifier.predict(X_test[i])\n",
        "                counter_Reasoning[1] += 1\n",
        "                counter_Reasoning[0] += not check_errors(G, prediction4[0], [\"Simplification\", \"Distraction\"], y_test[i])\n",
        "\n",
        "                #Simplification\n",
        "                if prediction4[0, 0] == 1:\n",
        "                    prediction6 = Simplification_classifier.predict(X_test[i])\n",
        "                    counter_Simplification[1] += 1\n",
        "                    counter_Simplification[0] += not check_errors(G, prediction6[0], [\"Causal Oversimplification\", \"Black-and-white Fallacy/Dictatorship\", \"Thought-terminating cliché\"], y_test[i])\n",
        "                    if prediction6[0, 0] == 1:\n",
        "                        y_pred[i].append(\"Causal Oversimplification\")\n",
        "                    if prediction6[0, 1] == 1:\n",
        "                        y_pred[i].append(\"Black-and-white Fallacy/Dictatorship\")\n",
        "                    if prediction6[0, 2] == 1:\n",
        "                        y_pred[i].append(\"Thought-terminating cliché\")\n",
        "                    if all(prediction6[0, i] == 0 for i in range(3)):\n",
        "                        y_pred[i].append(\"Simplification\")\n",
        "\n",
        "                #Distraction\n",
        "                if prediction4[0, 1] == 1:\n",
        "                    prediction7 = Distraction_classifier.predict(X_test[i])\n",
        "                    counter_Distraction[1] += 1\n",
        "                    counter_Distraction[0] += not check_errors(G, prediction7[0], [\"Whataboutism\",\"Misrepresentation of Someone's Position (Straw Man)\",\"Presenting Irrelevant Data (Red Herring)\"], y_test[i])\n",
        "                    if prediction7[0, 0] == 1:\n",
        "                        y_pred[i].append(\"Whataboutism\")\n",
        "                    if prediction7[0, 1] == 1:\n",
        "                        y_pred[i].append(\"Misrepresentation of Someone's Position (Straw Man)\")\n",
        "                    if prediction7[0, 2] == 1:\n",
        "                        y_pred[i].append(\"Presenting Irrelevant Data (Red Herring)\")\n",
        "                    if all(prediction7[0, i] == 0 for i in range(3)):\n",
        "                        y_pred[i].append(\"Distraction\")\n",
        "\n",
        "            #Justification\n",
        "            if prediction3[0, 3] == 1:\n",
        "                prediction5 = Justification_classifier.predict(X_test[i])\n",
        "                counter_Justification[1] += 1\n",
        "                counter_Justification[0] += not check_errors(G, prediction5[0], [\"Slogans\", \"Bandwagon\", \"Appeal to authority\", \"Flag-waving\", \"Appeal to fear/prejudice\"], y_test[i])\n",
        "                if prediction5[0, 0] == 1:\n",
        "                    y_pred[i].append(\"Slogans\")\n",
        "                if prediction5[0, 1] == 1:\n",
        "                    y_pred[i].append(\"Bandwagon\")\n",
        "                if prediction5[0, 2] == 1:\n",
        "                    y_pred[i].append(\"Appeal to authority\")\n",
        "                if prediction5[0, 3] == 1:\n",
        "                    y_pred[i].append(\"Flag-waving\")\n",
        "                if prediction5[0, 4] == 1:\n",
        "                    y_pred[i].append(\"Appeal to fear/prejudice\")\n",
        "                if all(prediction5[0, i] == 0 for i in range(5)):\n",
        "                    y_pred[i].append(\"Justification\")\n",
        "\n",
        "            if all(prediction3[0, i] == 0 for i in range(4)):\n",
        "                y_pred[i].append(\"Logos\")\n",
        "\n",
        "        #if all(prediction[0, i] == 0 for i in range(3)):\n",
        "        #    y_pred[i].append(\"Persuasion\")\n",
        "\n",
        "\n",
        "# Per evitare ripetizioni delle foglie raggiunguibili da più percorsi\n",
        "for i in range(len(y_pred)):\n",
        "    y_pred[i] = list(set(y_pred[i]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kyS3DGozlN2"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# Creazione di copie profonde per evitare riferimenti condivisi\n",
        "validation_data_aux = copy.deepcopy(test_data)\n",
        "\n",
        "# Sostituisci i valori del campo 'labels' di validation_data_aux con quelli di aux\n",
        "for i, element in enumerate(validation_data_aux):\n",
        "    element[\"labels\"] = y_pred[i]\n",
        "\n",
        "# Salva il nuovo validation_data modificato in un file pred.json\n",
        "output_file_path = \"pred4.json\"\n",
        "with open(output_file_path, \"w\", encoding='utf-8') as output_file:\n",
        "    json.dump(validation_data_aux, output_file, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K0Nb4OQ5J9t",
        "outputId": "9e22b354-41a0-417a-d4c2-be38f88fdb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1_h=0.45051\tprec_h=0.52090\trec_h=0.39688\n"
          ]
        }
      ],
      "source": [
        "import pdb\n",
        "import json\n",
        "import logging.handlers\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from networkx import DiGraph, relabel_nodes, all_pairs_shortest_path_length\n",
        "from sklearn_hierarchical_classification.constants import ROOT\n",
        "from sklearn_hierarchical_classification.metrics import h_fbeta_score, h_recall_score, h_precision_score, \\\n",
        "    fill_ancestors, multi_labeled\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "\n",
        "G = DiGraph()\n",
        "G.add_edge(ROOT, \"Logos\")\n",
        "G.add_edge(\"Logos\", \"Repetition\")\n",
        "G.add_edge(\"Logos\", \"Obfuscation, Intentional vagueness, Confusion\")\n",
        "G.add_edge(\"Logos\", \"Reasoning\")\n",
        "G.add_edge(\"Logos\", \"Justification\")\n",
        "G.add_edge('Justification', \"Slogans\")\n",
        "G.add_edge('Justification', \"Bandwagon\")\n",
        "G.add_edge('Justification', \"Appeal to authority\")\n",
        "G.add_edge('Justification', \"Flag-waving\")\n",
        "G.add_edge('Justification', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Reasoning', \"Simplification\")\n",
        "G.add_edge('Simplification', \"Causal Oversimplification\")\n",
        "G.add_edge('Simplification', \"Black-and-white Fallacy/Dictatorship\")\n",
        "G.add_edge('Simplification', \"Thought-terminating cliché\")\n",
        "G.add_edge('Reasoning', \"Distraction\")\n",
        "G.add_edge('Distraction', \"Misrepresentation of Someone's Position (Straw Man)\")\n",
        "G.add_edge('Distraction', \"Presenting Irrelevant Data (Red Herring)\")\n",
        "G.add_edge('Distraction', \"Whataboutism\")\n",
        "G.add_edge(ROOT, \"Ethos\")\n",
        "G.add_edge('Ethos', \"Appeal to authority\")\n",
        "G.add_edge('Ethos', \"Glittering generalities (Virtue)\")\n",
        "G.add_edge('Ethos', \"Bandwagon\")\n",
        "G.add_edge('Ethos', \"Ad Hominem\")\n",
        "G.add_edge('Ethos', \"Transfer\")\n",
        "G.add_edge('Ad Hominem', \"Doubt\")\n",
        "G.add_edge('Ad Hominem', \"Name calling/Labeling\")\n",
        "G.add_edge('Ad Hominem', \"Smears\")\n",
        "G.add_edge('Ad Hominem', \"Reductio ad hitlerum\")\n",
        "G.add_edge('Ad Hominem', \"Whataboutism\")\n",
        "G.add_edge(ROOT, \"Pathos\")\n",
        "G.add_edge('Pathos', \"Exaggeration/Minimisation\")\n",
        "G.add_edge('Pathos', \"Loaded Language\")\n",
        "G.add_edge('Pathos', \"Appeal to (Strong) Emotions\")\n",
        "G.add_edge('Pathos', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Pathos', \"Flag-waving\")\n",
        "G.add_edge('Pathos', \"Transfer\")\n",
        "\n",
        "KEYS = ['id','labels']\n",
        "logger = logging.getLogger(\"subtask_1_2a_scorer\")\n",
        "ch = logging.StreamHandler(sys.stdout)\n",
        "ch.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "ch.setFormatter(formatter)\n",
        "logger.setLevel(logging.INFO)\n",
        "#logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "def _read_gold_and_pred(pred_fpath, gold_fpath):\n",
        "  \"\"\"\n",
        "  Read gold and predicted data.\n",
        "  :param pred_fpath: a json file with predictions,\n",
        "  :param gold_fpath: the original annotated gold file.\n",
        "  :return: {id:pred_labels} dict; {id:gold_labels} dict\n",
        "  \"\"\"\n",
        "\n",
        "  gold_labels = {}\n",
        "  with open(gold_fpath, encoding='utf-8') as gold_f:\n",
        "    gold = json.load(gold_f)\n",
        "    for obj in gold:\n",
        "      gold_labels[obj['id']] = obj['labels']\n",
        "\n",
        "  pred_labels = {}\n",
        "  with open(pred_fpath, encoding='utf-8') as pred_f:\n",
        "    pred = json.load(pred_f)\n",
        "    for obj in pred:\n",
        "      pred_labels[obj['id']] = obj['labels']\n",
        "\n",
        "  if set(gold_labels.keys()) != set(pred_labels.keys()):\n",
        "      logger.error('There are either missing or added examples to the prediction file. Make sure you only have the gold examples in the prediction file.')\n",
        "      raise ValueError('There are either missing or added examples to the prediction file. Make sure you only have the gold examples in the prediction file.')\n",
        "\n",
        "  return pred_labels, gold_labels\n",
        "\n",
        "#F1\n",
        "def _h_fbeta_score(y_true, y_pred, class_hierarchy, beta=1., root=ROOT):\n",
        "    hP = _h_precision_score(y_true, y_pred, class_hierarchy, root=root)\n",
        "    hR = _h_recall_score(y_true, y_pred, class_hierarchy, root=root)\n",
        "    return (1. + beta ** 2.) * hP * hR / (beta ** 2. * hP + hR)\n",
        "\n",
        "def _fill_ancestors(y, graph, root, copy=True):\n",
        "    y_ = y.copy() if copy else y\n",
        "    paths = all_pairs_shortest_path_length(graph.reverse(copy=False))\n",
        "    for target, distances in paths:\n",
        "        if target == root:\n",
        "            continue\n",
        "        ix_rows = np.where(y[:, target] > 0)[0]\n",
        "        ancestors = list(filter(lambda x: x != ROOT,distances.keys()))\n",
        "        y_[tuple(np.meshgrid(ix_rows, ancestors))] = 1\n",
        "    graph.reverse(copy=False)\n",
        "    return y_\n",
        "def _h_recall_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
        "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
        "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
        "\n",
        "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
        "\n",
        "    true_positives = len(ix[0])\n",
        "    all_positives = np.count_nonzero(y_true_)\n",
        "\n",
        "    return true_positives / all_positives\n",
        "\n",
        "def _h_precision_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
        "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
        "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
        "\n",
        "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
        "\n",
        "    true_positives = len(ix[0])\n",
        "    all_results = np.count_nonzero(y_pred_)\n",
        "\n",
        "    return true_positives / all_results\n",
        "\n",
        "\n",
        "def evaluate_h(pred_fpath, gold_fpath):\n",
        "    pred_labels, gold_labels = _read_gold_and_pred(pred_fpath, gold_fpath)\n",
        "\n",
        "    gold = []\n",
        "    pred = []\n",
        "    for id in gold_labels:\n",
        "        gold.append(gold_labels[id])\n",
        "        pred.append(pred_labels[id])\n",
        "    with multi_labeled(gold, pred, G) as (gold_, pred_, graph_):\n",
        "        return  _h_precision_score(gold_, pred_,graph_), _h_recall_score(gold_, pred_,graph_), _h_fbeta_score(gold_, pred_,graph_)\n",
        "\n",
        "prec_h, rec_h, f1_h = evaluate_h(\"pred4.json\", \"dev_subtask1_en.json\")\n",
        "print(\"f1_h={:.5f}\\tprec_h={:.5f}\\trec_h={:.5f}\".format(f1_h, prec_h, rec_h))\n",
        "# 60 con mio albero\n",
        "# 54.536 loro albero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN6oTn0D1urz"
      },
      "source": [
        " ############################################## INIZIO SEZIONE TEST #################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqPdZE-_CESX"
      },
      "outputs": [],
      "source": [
        "# copia fatta nel main\n",
        "def check_errors(graph, binary_vector, predicted_labels, true_labels):\n",
        "    for pred_label, pred_value in zip(predicted_labels, binary_vector):\n",
        "        if pred_value == 1 and not any(nx.has_path(graph.reverse(), label, pred_label) for label in true_labels):\n",
        "            print(\"Errore nelle etichette predette:\", pred_label)\n",
        "            return False\n",
        "        elif pred_value == 0 and pred_label in true_labels:\n",
        "            print(\"Errore nelle etichette predette:\", pred_label)\n",
        "            return False\n",
        "\n",
        "    return True  # Se non ci sono errori, restituisce True\n",
        "\n",
        "\n",
        "a = check_errors(G, [0,1], [\"Free\", \"Persuasion\"], [\"Free\"])\n",
        "print(a)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}